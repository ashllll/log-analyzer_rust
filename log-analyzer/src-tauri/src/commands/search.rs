//! æœç´¢å‘½ä»¤å®ç°
//! åŒ…å«æ—¥å¿—æœç´¢åŠç¼“å­˜é€»è¾‘ï¼Œé™„å¸¦å…³é”®è¯ç»Ÿè®¡ä¸ç»“æœæ‰¹é‡æ¨é€

use std::panic::AssertUnwindSafe;
use parking_lot::Mutex;
use regex::Regex;
use sha2::{Digest, Sha256};
use std::{collections::HashSet, path::PathBuf, sync::Arc, time::Duration};
use tauri::{command, AppHandle, Emitter, State};
use tracing::{debug, error, warn};

// å¯¼å…¥AppErrorç±»å‹
use crate::error::AppError;

use crate::models::search::{QueryMetadata, QueryOperator, SearchTerm, TermSource};
use crate::models::search_statistics::SearchResultSummary;
use crate::models::{AppState, LogEntry, SearchCacheKey, SearchFilters, SearchQuery};
use crate::search_engine::manager::{SearchConfig, SearchEngineManager};
use crate::services::{calculate_keyword_statistics, parse_metadata, ExecutionPlan, QueryExecutor};

/// è®¡ç®—å¹¶æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
fn log_cache_statistics(total_searches: &Arc<Mutex<u64>>, cache_hits: &Arc<Mutex<u64>>) {
    let total = total_searches.lock();
    let hits = cache_hits.lock();
    let hit_rate = if *total > 0 {
        (*hits as f64 / *total as f64) * 100.0
    } else {
        0.0
    };
    println!(
        "[CACHE STATS] Total searches: {}, Cache hits: {}, Hit rate: {:.2}%",
        total, hits, hit_rate
    );
}

/// è®¡ç®—æŸ¥è¯¢å†…å®¹çš„å“ˆå¸Œç‰ˆæœ¬å·ï¼ˆç”¨äºç¼“å­˜é”®åŒºåˆ†ï¼‰
///
/// ä½¿ç”¨ SHA-256 å“ˆå¸Œç®—æ³•ç”ŸæˆæŸ¥è¯¢çš„ç‰ˆæœ¬æ ‡è¯†ç¬¦ï¼Œç¡®ä¿ä¸åŒæŸ¥è¯¢å†…å®¹
/// ä½¿ç”¨ä¸åŒçš„ç¼“å­˜é”®ï¼Œé¿å…ç¼“å­˜æ±¡æŸ“ã€‚
fn compute_query_version(query: &str) -> String {
    let mut hasher = Sha256::new();
    hasher.update(query.as_bytes());
    format!("{:x}", hasher.finalize())
}

/// è·å–æˆ–åˆå§‹åŒ– SearchEngineManager
///
/// ä½¿ç”¨å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å¼ï¼Œé¦–æ¬¡è°ƒç”¨æ—¶åˆ›å»ºç´¢å¼•
#[allow(dead_code)]
fn get_or_init_search_engine(state: &AppState, workspace_id: &str) -> Result<(), String> {
    let mut engine_guard = state.search_engine.lock();

    if engine_guard.is_none() {
        // åˆ›å»ºç´¢å¼•ç›®å½•è·¯å¾„
        let index_path = PathBuf::from(format!("./search_index/{}", workspace_id));

        // é…ç½®æœç´¢å¼•æ“
        let config = SearchConfig {
            default_timeout: Duration::from_millis(200),
            max_results: 50_000,
            index_path,
            writer_heap_size: 50_000_000, // 50MB
        };

        // åˆå§‹åŒ–æœç´¢å¼•æ“
        match SearchEngineManager::new(config) {
            Ok(engine) => {
                println!(
                    "[SEARCH ENGINE] Initialized Tantivy search engine for workspace: {}",
                    workspace_id
                );
                *engine_guard = Some(engine);
                Ok(())
            }
            Err(e) => {
                eprintln!("[SEARCH ENGINE] Failed to initialize: {}", e);
                Err(format!("Failed to initialize search engine: {}", e))
            }
        }
    } else {
        Ok(())
    }
}

#[command]
pub async fn search_logs(
    app: AppHandle,
    query: String,
    #[allow(non_snake_case)] workspaceId: Option<String>,
    max_results: Option<usize>,
    filters: Option<SearchFilters>,
    state: State<'_, AppState>,
) -> Result<String, String> {
    if query.is_empty() {
        return Err("Search query cannot be empty".to_string());
    }
    if query.len() > 1000 {
        return Err("Search query too long (max 1000 characters)".to_string());
    }

    let app_handle = app.clone();
    let workspace_dirs = Arc::clone(&state.workspace_dirs);
    let cas_instances = Arc::clone(&state.cas_instances);
    let metadata_stores = Arc::clone(&state.metadata_stores);
    let cache_manager = Arc::clone(&state.cache_manager);
    let total_searches = Arc::clone(&state.total_searches);
    let cache_hits = Arc::clone(&state.cache_hits);
    let last_search_duration = Arc::clone(&state.last_search_duration);
    let cancellation_tokens = Arc::clone(&state.search_cancellation_tokens);
    let metrics_collector = Arc::clone(&state.metrics_collector);

    let max_results = max_results.unwrap_or(50000).min(100_000);
    let filters = filters.unwrap_or_default();
    
    // ä¿®å¤å·¥ä½œåŒºIDå¤„ç†ï¼šå½“æ²¡æœ‰æä¾›workspaceIdæ—¶ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„å·¥ä½œåŒºè€Œä¸æ˜¯ç¡¬ç¼–ç çš„"default"
    let workspace_id = if let Some(ref id) = workspaceId {
        id.clone()
    } else {
        // å½“æ²¡æœ‰æä¾›å·¥ä½œåŒºIDæ—¶ï¼Œè·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„å·¥ä½œåŒº
        let dirs = workspace_dirs.lock();
        if let Some(first_workspace_id) = dirs.keys().next() {
            debug!(
                workspace_id = %first_workspace_id,
                available_workspaces = ?dirs.keys().collect::<Vec<_>>(),
                "Using first available workspace as default"
            );
            first_workspace_id.clone()
        } else {
            // å¦‚æœæ²¡æœ‰å¯ç”¨çš„å·¥ä½œåŒºï¼Œè¿”å›æ˜ç¡®çš„é”™è¯¯
            let _ = app.emit("search-error", "No workspaces available. Please create a workspace first.");
            return Err("No workspaces available".to_string());
        }
    };

    // ç”Ÿæˆå”¯ä¸€çš„æœç´¢ID
    let search_id = uuid::Uuid::new_v4().to_string();

    // åˆ›å»ºå–æ¶ˆä»¤ç‰Œ
    let cancellation_token = tokio_util::sync::CancellationToken::new();
    {
        let mut tokens = cancellation_tokens.lock();
        tokens.insert(search_id.clone(), cancellation_token.clone());
    }

    // ç¼“å­˜é”®ï¼šåŸºäºæŸ¥è¯¢å‚æ•°ç”Ÿæˆï¼Œä½¿ç”¨æŸ¥è¯¢å†…å®¹çš„å“ˆå¸Œä½œä¸ºç‰ˆæœ¬å·
    // ä½¿ç”¨ SHA-256 å“ˆå¸Œç¡®ä¿ä¸åŒæŸ¥è¯¢ä½¿ç”¨ä¸åŒç¼“å­˜é”®ï¼Œé¿å…ç¼“å­˜æ±¡æŸ“
    let query_version = compute_query_version(&query);
    let cache_key: SearchCacheKey = (
        query.clone(),
        workspace_id.clone(),
        filters.time_start.clone(),
        filters.time_end.clone(),
        filters.levels.clone(),
        filters.file_pattern.clone(),
        false, // case_sensitive - éœ€è¦ä»æŸ¥è¯¢ä¸­è·å–
        max_results,
        query_version, // ä½¿ç”¨ SHA-256 å“ˆå¸Œä½œä¸ºç‰ˆæœ¬å·
    );

    {
        // ä½¿ç”¨ CacheManager çš„åŒæ­¥ get æ–¹æ³•
        let cache_result = state.cache_manager.get_sync(&cache_key);

        if let Some(cached_results) = cache_result {
            {
                let mut hits = cache_hits.lock();
                *hits += 1;
            }
            {
                let mut searches = total_searches.lock();
                *searches += 1;
            }

            // è®°å½•ç¼“å­˜ç»Ÿè®¡
            log_cache_statistics(&total_searches, &cache_hits);

            // å‘é€ç¼“å­˜ç»“æœï¼ˆæ‰¹é‡å‘é€ï¼Œä¸ä½¿ç”¨ sleep é˜»å¡ï¼‰
            for chunk in cached_results.chunks(500) {
                let _ = app_handle.emit("search-results", chunk);
                // ç§»é™¤ thread::sleepï¼Œä½¿ç”¨ tokio::task::yield_now é¿å…é˜»å¡
                // ä½†ç”±äºåœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­ï¼Œç›´æ¥å‘é€å³å¯
            }

            let raw_terms: Vec<String> = query
                .split('|')
                .map(|t| t.trim())
                .filter(|t| !t.is_empty())
                .map(|t| t.to_string())
                .collect();

            let keyword_stats = calculate_keyword_statistics(&cached_results, &raw_terms);
            let summary = SearchResultSummary::new(cached_results.len(), keyword_stats, 0, false);

            let _ = app_handle.emit("search-summary", &summary);
            let _ = app_handle.emit("search-complete", cached_results.len());
            return Ok(search_id);
        }
    }

    {
        let mut searches = total_searches.lock();
        *searches += 1;
    }

    let search_id_clone = search_id.clone();
    // è€ç‹å¤‡æ³¨ï¼šä¿®å¤çº¿ç¨‹æ³„æ¼ï¼ä½¿ç”¨tokio::task::spawn_blockingä»£æ›¿std::thread::spawn
    // è¿™æ ·tokioè¿è¡Œæ—¶ä¼šç®¡ç†çº¿ç¨‹ç”Ÿå‘½å‘¨æœŸï¼Œé¿å…èµ„æºæ³„æ¼
    let _handle = tokio::task::spawn_blocking(move || {
        let start_time = std::time::Instant::now();
        let parse_start = std::time::Instant::now();

        let raw_terms: Vec<String> = query
            .split('|')
            .map(|t| t.trim())
            .filter(|t| !t.is_empty())
            .map(|t| t.to_string())
            .collect();

        if raw_terms.is_empty() {
            let _ = app_handle.emit("search-error", "Search query is empty after processing");
            return;
        }

        let search_terms: Vec<SearchTerm> = raw_terms
            .iter()
            .enumerate()
            .map(|(i, term)| SearchTerm {
                id: format!("term_{}", i),
                value: term.clone(),
                operator: QueryOperator::Or,
                source: TermSource::User,
                preset_group_id: None,
                is_regex: false,
                priority: 1,
                enabled: true,
                case_sensitive: false,
            })
            .collect();

        let structured_query = SearchQuery {
            id: "search_logs_query".to_string(),
            terms: search_terms,
            global_operator: QueryOperator::Or,
            filters: None,
            metadata: QueryMetadata {
                created_at: 0,
                last_modified: 0,
                execution_count: 0,
                label: None,
            },
        };

        let parse_duration = parse_start.elapsed();
// ============================================================        // é«˜çº§æœç´¢ç‰¹æ€§é›†æˆç‚¹        // ============================================================        // FilterEngine: ä½å›¾ç´¢å¼•åŠ é€Ÿè¿‡æ»¤ï¼ˆ10Kæ–‡æ¡£ < 10msï¼‰        // RegexSearchEngine: LRUç¼“å­˜æ­£åˆ™æœç´¢ï¼ˆåŠ é€Ÿ50x+ï¼‰        // TimePartitionedIndex: æ—¶é—´åˆ†åŒºç´¢å¼•ï¼ˆæ—¶åºæŸ¥è¯¢ä¼˜åŒ–ï¼‰        // AutocompleteEngine: Trieæ ‘è‡ªåŠ¨è¡¥å…¨ï¼ˆ< 100mså“åº”ï¼‰        //         // ä½¿ç”¨æ–¹å¼ï¼š        // 1. ä» AppState è·å–é«˜çº§ç‰¹æ€§å®ä¾‹ï¼ˆå·²åˆå§‹åŒ–ï¼‰        // 2. åœ¨æœç´¢å‰ä½¿ç”¨ FilterEngine é¢„è¿‡æ»¤å€™é€‰æ–‡æ¡£        // 3. åœ¨è¿‡æ»¤æ—¶ä½¿ç”¨ RegexSearchEngine åŠ é€Ÿæ­£åˆ™åŒ¹é…        // 4. åœ¨æ—¶é—´èŒƒå›´æŸ¥è¯¢æ—¶ä½¿ç”¨ TimePartitionedIndex        //         // é…ç½®å¼€å…³ï¼šconfig.json -> advanced_features.enable_*        tracing::info!("ğŸ” é«˜çº§æœç´¢ç‰¹æ€§å·²å°±ç»ªï¼ˆå¯é€šè¿‡é…ç½®å¯ç”¨ï¼‰");

        let execution_start = std::time::Instant::now();
        let mut executor = QueryExecutor::new(100);
        let plan = match executor.execute(&structured_query) {
            Ok(p) => p,
            Err(e) => {
                let _ = app_handle.emit("search-error", format!("Query execution error: {}", e));
                return;
            }
        };

        // Get workspace directory
        let workspace_dir = {
            let dirs = workspace_dirs.lock();
            debug!(
                workspace_id = %workspace_id,
                available_workspaces = ?dirs.keys().collect::<Vec<_>>(),
                "Looking up workspace directory"
            );
            match dirs.get(&workspace_id) {
                Some(dir) => {
                    debug!(
                        workspace_id = %workspace_id,
                        directory = %dir.display(),
                        "Found workspace directory"
                    );
                    dir.clone()
                },
                None => {
                    error!(
                        workspace_id = %workspace_id,
                        available_workspaces = ?dirs.keys().collect::<Vec<_>>(),
                        "Workspace directory not found"
                    );
                    
                    // å¦‚æœæ˜¯"default"å·¥ä½œåŒºï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„å·¥ä½œåŒº
                    if workspace_id == "default" {
                        if let Some(first_workspace_id) = dirs.keys().next() {
                            debug!(
                                workspace_id = %first_workspace_id,
                                "Falling back to first available workspace instead of 'default'"
                            );
                            let _ = app_handle.emit("search-error", format!("Workspace 'default' not found, using '{}' instead", first_workspace_id));
                            return;
                        }
                    }
                    
                    let _ = app_handle.emit(
                        "search-error",
                        format!("Workspace directory not found for: {}", workspace_id),
                    );
                    return;
                }
            }
        };

        // Get or create MetadataStore for this workspace
        let metadata_store = {
            let mut stores = metadata_stores.lock();
            if let Some(store) = stores.get(&workspace_id) {
                Arc::clone(store)
            } else {
                // Create new MetadataStore using block_in_place for async operation
                // æ·»åŠ é”™è¯¯å¤„ç†é˜²æ­¢panic
                let store_result = match std::panic::catch_unwind(AssertUnwindSafe(|| {
                    tokio::task::block_in_place(|| {
                        match tokio::runtime::Handle::try_current() {
                            Ok(handle) => {
                                debug!(
                                    workspace_id = %workspace_id,
                                    directory = %workspace_dir.display(),
                                    "Creating new MetadataStore with Tokio runtime"
                                );
                                handle.block_on(
                                    crate::storage::metadata_store::MetadataStore::new(&workspace_dir),
                                )
                            }
                            Err(e) => {
                                error!(
                                    workspace_id = %workspace_id,
                                    directory = %workspace_dir.display(),
                                    error = %e,
                                    "Failed to acquire Tokio runtime handle for MetadataStore creation"
                                );
                                // è¿”å›é”™è¯¯è€Œä¸æ˜¯panicï¼Œéœ€è¦è½¬æ¢ä¸ºAppErrorç±»å‹
                                Err(AppError::DatabaseError(format!("Tokio runtime error: {}", e)))
                            }
                        }
                    })
                })) {
                    Ok(result) => result,
                    Err(panic_info) => {
                        error!(
                            workspace_id = %workspace_id,
                            directory = %workspace_dir.display(),
                            panic_info = ?panic_info,
                            "Panic occurred while creating MetadataStore"
                        );
                        Err(AppError::DatabaseError("Internal error occurred while creating metadata store".to_string()))
                    }
                };

                match store_result {
                    Ok(store) => {
                        let store_arc = Arc::new(store);
                        stores.insert(workspace_id.clone(), Arc::clone(&store_arc));
                        store_arc
                    }
                    Err(e) => {
                        let _ = app_handle.emit(
                            "search-error",
                            format!("Failed to open metadata store: {}", e),
                        );
                        return;
                    }
                }
            }
        };

        // Get or create CAS for this workspace
        let cas = {
            let mut instances = cas_instances.lock();
            if let Some(cas) = instances.get(&workspace_id) {
                Arc::clone(cas)
            } else {
                // Create new CAS instance
                let cas_arc = Arc::new(crate::storage::ContentAddressableStorage::new(
                    workspace_dir.clone(),
                ));
                instances.insert(workspace_id.clone(), Arc::clone(&cas_arc));
                cas_arc
            }
        };

        // Get all files from MetadataStore (Requirements 2.3) using block_in_place
        // æ·»åŠ é”™è¯¯å¤„ç†é˜²æ­¢panic
        let files = match std::panic::catch_unwind(AssertUnwindSafe(|| {
            tokio::task::block_in_place(|| {
                // æ£€æŸ¥Tokioè¿è¡Œæ—¶æ˜¯å¦å¯ç”¨
                match tokio::runtime::Handle::try_current() {
                    Ok(handle) => {
                        debug!(
                            workspace_id = %workspace_id,
                            "Successfully acquired Tokio runtime handle"
                        );
                        handle.block_on(metadata_store.get_all_files())
                    }
                    Err(e) => {
                        error!(
                            workspace_id = %workspace_id,
                            error = %e,
                            "Failed to acquire Tokio runtime handle"
                        );
                        // è¿”å›ç©ºç»“æœè€Œä¸æ˜¯panic
                        Ok(Vec::new())
                    }
                }
            })
        })) {
            Ok(result) => result,
            Err(panic_info) => {
                error!(
                    workspace_id = %workspace_id,
                    panic_info = ?panic_info,
                    "Panic occurred while getting files from metadata store"
                );
                let _ = app_handle.emit(
                    "search-error",
                    format!("Internal error occurred while accessing workspace: {}", workspace_id),
                );
                return;
            }
        };

        let files = match files {
            Ok(files) => files,
            Err(e) => {
                let _ = app_handle.emit(
                    "search-error",
                    format!("Failed to get files from metadata store: {}", e),
                );
                return;
            }
        };

        debug!(
            total_files = files.len(),
            workspace_id = %workspace_id,
            "Starting search across files using CAS"
        );

        // æµå¼å¤„ç†ï¼šåˆ†æ‰¹å‘é€ç»“æœï¼Œé¿å…å†…å­˜å³°å€¼
        let batch_size = 500;
        let mut total_processed = 0;
        let mut results_count = 0;
        // æµå¼ç»Ÿè®¡ï¼šä½¿ç”¨HashMapå¢é‡ç»Ÿè®¡å…³é”®è¯ï¼Œé¿å…ç´¯ç§¯æ‰€æœ‰ç»“æœ
        let mut keyword_counts: std::collections::HashMap<String, usize> =
            std::collections::HashMap::new();
        let mut was_truncated = false;
        let mut pending_batch: Vec<LogEntry> = Vec::new(); // å½“å‰å¾…å‘é€æ‰¹æ¬¡
        let mut all_results: Vec<LogEntry> = Vec::new(); // ç”¨äºç¼“å­˜çš„å®Œæ•´ç»“æœé›†

        // å…ˆå‘é€å¼€å§‹äº‹ä»¶
        let _ = app_handle.emit("search-start", "Starting search...");

        'outer: for file_batch in files.chunks(10) {
            // æ£€æŸ¥å–æ¶ˆçŠ¶æ€
            if cancellation_token.is_cancelled() {
                let _ = app_handle.emit("search-cancelled", search_id_clone.clone());
                // æ¸…ç†å–æ¶ˆä»¤ç‰Œ
                {
                    let mut tokens = cancellation_tokens.lock();
                    tokens.remove(&search_id_clone);
                }
                return;
            }

            // æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°max_resultsé™åˆ¶
            if results_count >= max_results {
                was_truncated = true;
                break 'outer;
            }

            // æ¯æ‰¹å¤„ç†10ä¸ªæ–‡ä»¶
            let mut batch_results: Vec<LogEntry> = Vec::new();

            // å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡ (Requirements 2.3: ä½¿ç”¨ CAS è¯»å–å†…å®¹)
            let batch: Vec<_> = file_batch
                .iter()
                .enumerate()
                .map(|(idx, file_metadata)| {
                    // Use CAS-based access with hash
                    let file_identifier = format!("cas://{}", file_metadata.sha256_hash);
                    search_single_file_with_details(
                        &file_identifier,
                        &file_metadata.virtual_path,
                        Some(&*cas), // Pass CAS instance for hash-based access
                        &executor,
                        &plan,
                        total_processed + idx * 10000,
                    )
                })
                .collect();

            // æ”¶é›†å½“å‰æ‰¹æ¬¡çš„ç»“æœ
            for file_results in batch {
                for entry in file_results {
                    // æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°max_resultsé™åˆ¶
                    if results_count >= max_results {
                        was_truncated = true;
                        break 'outer;
                    }

                    // åº”ç”¨è¿‡æ»¤å™¨
                    let mut include = true;

                    if !filters.levels.is_empty() && !filters.levels.contains(&entry.level) {
                        include = false;
                    }
                    if include && filters.time_start.is_some() {
                        if let Some(ref start) = filters.time_start {
                            if entry.timestamp < *start {
                                include = false;
                            }
                        }
                    }
                    if include && filters.time_end.is_some() {
                        if let Some(ref end) = filters.time_end {
                            if entry.timestamp > *end {
                                include = false;
                            }
                        }
                    }
                    if include && filters.file_pattern.is_some() {
                        if let Some(ref pattern) = filters.file_pattern {
                            if !entry.file.contains(pattern) && !entry.real_path.contains(pattern) {
                                include = false;
                            }
                        }
                    }

                    if include {
                        // æµå¼ç»Ÿè®¡ï¼šå¢é‡æ›´æ–°å…³é”®è¯è®¡æ•°
                        if let Some(ref keywords) = entry.matched_keywords {
                            for kw in keywords {
                                *keyword_counts.entry(kw.clone()).or_insert(0) += 1;
                            }
                        }

                        // ä¿å­˜åˆ°å®Œæ•´ç»“æœé›†ç”¨äºç¼“å­˜
                        all_results.push(entry.clone());
                        batch_results.push(entry);
                        results_count += 1;

                        // å½“æ‰¹æ¬¡æ»¡æ—¶å‘é€
                        if batch_results.len() >= batch_size {
                            let _ = app_handle.emit("search-results", &batch_results);
                            batch_results.clear();
                        }
                    }
                }
            }

            // ä¿å­˜æœªå‘é€çš„ç»“æœ
            if !batch_results.is_empty() {
                pending_batch = batch_results;
            }

            total_processed += file_batch.len();

            // å‘é€è¿›åº¦æ›´æ–°
            let progress = (total_processed as f64 / files.len() as f64 * 100.0) as i32;
            let _ = app_handle.emit("search-progress", progress);
        }

        // å‘é€å‰©ä½™ç»“æœ
        if !pending_batch.is_empty() {
            let _ = app_handle.emit("search-results", &pending_batch);
        }

        // è®¡ç®—æœç´¢ç»Ÿè®¡ä¿¡æ¯
        let duration = start_time.elapsed().as_millis() as u64;
        {
            let mut last_duration = last_search_duration.lock();
            *last_duration = duration;
        }

        let execution_duration = execution_start.elapsed();
        let format_duration = start_time.elapsed() - parse_duration - execution_duration;

        // è®°å½•æ€§èƒ½æŒ‡æ ‡
        use crate::monitoring::metrics_collector::SearchPhase;
        let phase_timings = vec![
            (SearchPhase::Parsing, parse_duration),
            (SearchPhase::Execution, execution_duration),
            (SearchPhase::Formatting, format_duration),
        ];

        metrics_collector.record_search_operation(
            &query,
            results_count,
            start_time.elapsed(),
            phase_timings,
            !was_truncated && !cancellation_token.is_cancelled(),
        );

        // ä½¿ç”¨æµå¼ç»Ÿè®¡ç»“æœæ„å»ºå…³é”®è¯ç»Ÿè®¡
        let keyword_stats: Vec<crate::models::search_statistics::KeywordStatistics> = raw_terms
            .iter()
            .map(|term| {
                let count = keyword_counts.get(term).copied().unwrap_or(0);
                crate::models::search_statistics::KeywordStatistics::new(
                    term.clone(),
                    count,
                    results_count,
                )
            })
            .collect();

        let summary = SearchResultSummary::new(
            results_count,
            keyword_stats,
            duration,
            was_truncated, // æ ‡è®°æ˜¯å¦å› è¾¾åˆ°é™åˆ¶è€Œæˆªæ–­
        );

        // å°†ç»“æœæ’å…¥ç¼“å­˜(ä»…åœ¨æœªæˆªæ–­ä¸”æœªå–æ¶ˆæ—¶ç¼“å­˜)
        if !was_truncated && !cancellation_token.is_cancelled() {
            cache_manager.insert_sync(cache_key, all_results);
        }

        let _ = app_handle.emit("search-summary", &summary);
        let _ = app_handle.emit("search-complete", results_count);

        // æ¸…ç†å–æ¶ˆä»¤ç‰Œ
        {
            let mut tokens = cancellation_tokens.lock();
            tokens.remove(&search_id_clone);
        }
    });

    Ok(search_id)
}

/// Legacy search function using regex (kept for backward compatibility)
///
/// This function is marked as dead_code but kept for potential future use.
/// New code should use `search_single_file_with_details` instead.
#[allow(dead_code)]
fn search_single_file(
    sha256_hash: &str,
    virtual_path: &str,
    cas: &crate::storage::ContentAddressableStorage,
    re: &Regex,
    global_offset: usize,
) -> Vec<LogEntry> {
    let mut results = Vec::new();

    // Read content from CAS using hash
    let content = match tokio::task::block_in_place(|| {
        tokio::runtime::Handle::current().block_on(cas.read_content(sha256_hash))
    }) {
        Ok(bytes) => bytes,
        Err(e) => {
            warn!(
                hash = %sha256_hash,
                virtual_path = %virtual_path,
                error = %e,
                "Failed to read content from CAS"
            );
            return results;
        }
    };

    // Convert bytes to string
    let content_str = match String::from_utf8(content) {
        Ok(s) => s,
        Err(e) => {
            warn!(
                hash = %sha256_hash,
                virtual_path = %virtual_path,
                error = %e,
                "File content is not valid UTF-8, skipping"
            );
            return results;
        }
    };

    // Process lines
    for (i, line) in content_str.lines().enumerate() {
        if re.is_match(line) {
            let (ts, lvl) = parse_metadata(line);
            results.push(LogEntry {
                id: global_offset + i,
                timestamp: ts,
                level: lvl,
                file: virtual_path.to_string(),
                real_path: format!("cas://{}", sha256_hash),
                line: i + 1,
                content: line.to_string(),
                tags: vec![],
                match_details: None,
                matched_keywords: None,
            });
        }
    }

    results
}

/// å–æ¶ˆæ­£åœ¨è¿›è¡Œçš„æœç´¢
#[command]
pub async fn cancel_search(
    #[allow(non_snake_case)] searchId: String,
    state: State<'_, AppState>,
) -> Result<(), String> {
    let cancellation_tokens = Arc::clone(&state.search_cancellation_tokens);

    let token = {
        let tokens = cancellation_tokens.lock();
        tokens.get(&searchId).cloned()
    };

    if let Some(token) = token {
        token.cancel();
        Ok(())
    } else {
        Err(format!(
            "Search with ID {} not found or already completed",
            searchId
        ))
    }
}

/// Search a single file with support for both path-based and hash-based access
///
/// This function supports two modes:
/// 1. Hash-based (CAS): When `file_identifier` starts with "cas://", reads from CAS
/// 2. Path-based (legacy): When `file_identifier` is a file path, reads from filesystem
///
/// # Arguments
///
/// * `file_identifier` - Either "cas://<hash>" for CAS access or a file path for legacy access
/// * `virtual_path` - Virtual path for display purposes
/// * `cas_opt` - Optional Content-Addressable Storage instance (required for CAS mode)
/// * `executor` - Query executor for matching
/// * `plan` - Execution plan for the query
/// * `global_offset` - Offset for line numbering
///
/// # Returns
///
/// Vector of matching log entries
fn search_single_file_with_details(
    file_identifier: &str,
    virtual_path: &str,
    cas_opt: Option<&crate::storage::ContentAddressableStorage>,
    executor: &QueryExecutor,
    plan: &ExecutionPlan,
    global_offset: usize,
) -> Vec<LogEntry> {
    let mut results = Vec::new();

    // Determine if this is CAS-based or path-based access
    if let Some(sha256_hash) = file_identifier.strip_prefix("cas://") {
        // Hash-based access via CAS

        let cas = match cas_opt {
            Some(c) => c,
            None => {
                error!(
                    hash = %sha256_hash,
                    virtual_path = %virtual_path,
                    "CAS instance not provided for hash-based access"
                );
                return results;
            }
        };

        // Verify hash exists in CAS before reading (Requirements 8.1, 8.3)
        if !cas.exists(sha256_hash) {
            warn!(
                hash = %sha256_hash,
                virtual_path = %virtual_path,
                "Hash does not exist in CAS, skipping file"
            );
            return results;
        }

        // Read content from CAS using hash
        let content = match tokio::task::block_in_place(|| {
            tokio::runtime::Handle::current().block_on(cas.read_content(sha256_hash))
        }) {
            Ok(bytes) => bytes,
            Err(e) => {
                warn!(
                    hash = %sha256_hash,
                    virtual_path = %virtual_path,
                    error = %e,
                    "Failed to read content from CAS, skipping file"
                );
                return results;
            }
        };

        // Convert bytes to string
        let content_str = match String::from_utf8(content) {
            Ok(s) => s,
            Err(e) => {
                warn!(
                    hash = %sha256_hash,
                    virtual_path = %virtual_path,
                    error = %e,
                    "File content is not valid UTF-8, skipping"
                );
                return results;
            }
        };

        // Process lines
        for (i, line) in content_str.lines().enumerate() {
            if executor.matches_line(plan, line) {
                let (ts, lvl) = parse_metadata(line);
                let match_details = executor.match_with_details(plan, line);
                let matched_keywords = match_details.as_ref().map(|details| {
                    details
                        .iter()
                        .map(|detail| detail.term_value.clone())
                        .collect::<HashSet<_>>()
                        .into_iter()
                        .collect::<Vec<_>>()
                });

                results.push(LogEntry {
                    id: global_offset + i,
                    timestamp: ts,
                    level: lvl,
                    file: virtual_path.to_string(),
                    real_path: file_identifier.to_string(),
                    line: i + 1,
                    content: line.to_string(),
                    tags: vec![],
                    match_details,
                    matched_keywords: matched_keywords.filter(|v| !v.is_empty()),
                });
            }
        }

        debug!(
            hash = %sha256_hash,
            virtual_path = %virtual_path,
            matches = results.len(),
            "Searched file via CAS"
        );
    } else {
        // Legacy path-based access
        use std::fs::File;
        use std::io::{BufRead, BufReader};
        use std::path::Path;

        let real_path = file_identifier;
        let path = Path::new(real_path);

        if !path.exists() {
            warn!(
                file = %real_path,
                "Skipping non-existent file"
            );
            return results;
        }

        match File::open(real_path) {
            Ok(file) => {
                let reader = BufReader::with_capacity(8192, file);

                for (i, line_res) in reader.lines().enumerate() {
                    if let Ok(line) = line_res {
                        if executor.matches_line(plan, &line) {
                            let (ts, lvl) = parse_metadata(&line);
                            let match_details = executor.match_with_details(plan, &line);
                            let matched_keywords = match_details.as_ref().map(|details| {
                                details
                                    .iter()
                                    .map(|detail| detail.term_value.clone())
                                    .collect::<HashSet<_>>()
                                    .into_iter()
                                    .collect::<Vec<_>>()
                            });

                            results.push(LogEntry {
                                id: global_offset + i,
                                timestamp: ts,
                                level: lvl,
                                file: virtual_path.to_string(),
                                real_path: real_path.to_string(),
                                line: i + 1,
                                content: line,
                                tags: vec![],
                                match_details,
                                matched_keywords: matched_keywords.filter(|v| !v.is_empty()),
                            });
                        }
                    }
                }

                debug!(
                    path = %real_path,
                    virtual_path = %virtual_path,
                    matches = results.len(),
                    "Searched file via filesystem"
                );
            }
            Err(e) => {
                error!(
                    file = %real_path,
                    error = %e,
                    "Failed to open file for search"
                );
            }
        }
    }

    results
}
